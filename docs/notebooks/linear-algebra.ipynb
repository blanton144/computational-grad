{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations  in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define a vector and matrix, we can multiply them explicitly, but it is better to use NumPy routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30000\n",
    "A = np.zeros((n, n))\n",
    "x = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit(A=None, x=None):\n",
    "    n = A.shape[0]\n",
    "    b = np.zeros(n)\n",
    "    for i in np.arange(n):\n",
    "        b[i] = (A[i, :] * x).sum()\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit b = explicit(A=A, x=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit b = A.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also make outer products and other operations with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(100)\n",
    "q = np.zeros(50)\n",
    "y = np.outer(x, q)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the numpy.linalg module contains a plethora of other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((100, 100))\n",
    "print(linalg.det(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((6, 10, 5))\n",
    "y = np.zeros((10))\n",
    "z = np.zeros((5))\n",
    "w = np.einsum('ijk,j,k->i', x, y, z)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving a linear system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "A = np.random.random((20, 20))\n",
    "x_actual = np.random.random(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_actual.reshape((20, 1)))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = A.dot(x_actual)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = linalg.solve(A, b).reshape(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x - x_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also solve this by explicitly taking the inverse. If we are going to need to solve the equation more than $N$ times, this becomes worthwhile, because we can just calculate and save the inverse, and then apply it, rather than performing the backsubstitution explicitly every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv = linalg.inv(A)\n",
    "x = Ainv.dot(b)\n",
    "print(x - x_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Application of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore how a matrix works as a linear mapping, and how SVD tells us about that linear mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((2, 2))\n",
    "A[0, 0] = 1\n",
    "A[0, 1] = 0.5\n",
    "A[1, 0] = 0.5  \n",
    "A[1, 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take two basis vectors for the space of $\\vec{x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1., 0])\n",
    "x1 = np.array([0., 1])\n",
    "origin = np.array([0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.arrow(origin[0], origin[1], x0[0], x0[1], color='blue')\n",
    "plt.arrow(origin[0], origin[1], x1[0], x1[1], color='red')\n",
    "plt.xlim((-0.1, 1.3))\n",
    "plt.ylim((-0.1, 1.3))\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = A.dot(x0)\n",
    "b1 = A.dot(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.arrow(origin[0], origin[1], b0[0], b0[1], color='blue')\n",
    "plt.arrow(origin[0], origin[1], b1[0], b1[1], color='red')\n",
    "plt.xlim((-0.1, 1.3))\n",
    "plt.ylim((-0.1, 1.3))\n",
    "plt.xlabel('$b_0$')\n",
    "plt.ylabel('$b_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(U, w, VT) = linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv = VT.transpose().dot(np.diag(1. / w)).dot(U.transpose())\n",
    "print(Ainv.dot(b0))\n",
    "print(Ainv.dot(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(2, 1000))\n",
    "b = A.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[0, :], x[1, :], '.')\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(b[0, :], b[1, :], '.')\n",
    "plt.xlabel(\"$b_0$\")\n",
    "plt.ylabel(\"$b_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple singular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((2, 2))\n",
    "A[0, 0] = 1\n",
    "A[0, 1] = -1\n",
    "A[1, 0] = -1\n",
    "A[1, 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clearly the range of $A$ by throwing a bunch of random points at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(2, 1000))\n",
    "x2 = np.random.normal(size=(2, 1000))\n",
    "xr = np.random.random(size=1000) * 10000.\n",
    "x2[0, :] = x[0, :]+xr\n",
    "x2[1, :] = x[1, :]+xr\n",
    "b = A.dot(x)\n",
    "b2 = A.dot(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[0, :] , x[1, :], '.')\n",
    "plt.plot(x2[0, :] , x2[1, :], '.')\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(b[0, :], b[1, :], '.')\n",
    "plt.plot(b2[0, :], b2[1, :], '.')\n",
    "plt.xlabel(\"$b_0$\")\n",
    "plt.ylabel(\"$b_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sufficient, in fact, to look just at what happens to the basis vectors (all other results are just a linear combination of those)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1., 0])\n",
    "x1 = np.array([0., 1])\n",
    "origin = np.array([0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = A.dot(x0)\n",
    "b1 = A.dot(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.arrow(origin[0], origin[1], b0[0], b0[1], color='blue')\n",
    "plt.arrow(origin[0], origin[1], b1[0], b1[1], color='red')\n",
    "plt.xlim((-1.3, 1.3))\n",
    "plt.ylim((-1.3, 1.3))\n",
    "plt.xlabel('$b_0$')\n",
    "plt.ylabel('$b_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD gives us a diagnosis; for much higher dimensional problems this is very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(U, w, VT) = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this results in a zero component of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of ${\\bf U}$ corresponding to non-zero $w_j$ give the range of ${\\bf A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U)\n",
    "print(U[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the columns of ${\\bf V}$ corresponding to zero $w_j$ give the null space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VT.transpose()[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how SVD works on some simple examples. If $\\vec{b} = (-0.5, 0.6)$, we should expect to get an $\\vec{x}$ that gets us as close as we can get to this within the range of ${\\bf A}$. It will not get us exactly there however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winv = np.zeros(len(w))\n",
    "indx = np.where(w > w.max() * 1.e-15)[0]\n",
    "winv[indx] = 1. / w[indx]\n",
    "print(winv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ainv = VT.transpose().dot(np.diag(winv)).dot(U.transpose()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([-0.5, 0.6])\n",
    "x = Ainv.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\bf A}\\cdot\\vec{x}$ yields some approximation of $\\vec{b}$, that lies in the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bapprox = print(A.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\vec{b}$ is in the range, e.g. is $(1, -1)$, it can be recovered exactly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([1., -1])\n",
    "x = Ainv.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bapprox = print(A.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the null space allows me to add any vector with $x_0 = x_1$ to the above solution. The given $\\vec{x}$ is the choice which minimizes its norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SVD to solve linear regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.random(size=1000) * 5. - 2.5\n",
    "b = 0.5 + 1.2 * t + 0.43 * t**2 + 0.314 * t**3 + 2. * np.random.normal(size=len(t))\n",
    "plt.plot(t, b, '.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((len(t), 4))\n",
    "A[:, 0] = 1.\n",
    "A[:, 1] = t \n",
    "A[:, 2] = t**2\n",
    "A[:, 3] = t**3\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(u, w, vt) = np.linalg.svd(A, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u.shape)\n",
    "print(w.shape)\n",
    "print(vt.shape)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainv = vt.transpose().dot(np.diag(1. / w)).dot(u.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ainv.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = A.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, b, '.', label='data')\n",
    "plt.plot(t, bm, '.', label='model')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(t, b - bm, '.', label='data - model')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('$\\Delta$b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the dependent variable to better condition matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, $x$ took on values of order unity. But you have to be careful in problems like this to avoid roundoff issues that can occur if you set up the problem incorrectly. \n",
    "\n",
    "To demonstrate this issue, we are going to fit at slightly higher order, because the problem arises more quickly in that case. \n",
    "\n",
    "But the key thing that happens here is that we are using a dependent variable that gets to high values, and which we take a large power of. This will lead to a design matrix $A$ whose components have a wide dynamic range, which will lead it to be ill-conditioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.random(size=10000) * 10000. + 10000.\n",
    "b = 40. + 0.1e-3 * t - 0.8e-6 * t**2 + 0.03e-9 * t**3 + 2.e-16 * t**4 + 1.e-20 * t**5 + 2. * np.random.normal(size=len(x))\n",
    "plt.plot(t, b, '.')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, we are fitting a higher order polynomial in this case; a fifth order one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((len(t), 6))\n",
    "A[:, 0] = 1.\n",
    "A[:, 1] = t \n",
    "A[:, 2] = t**2\n",
    "A[:, 3] = t**3\n",
    "A[:, 4] = t**4\n",
    "A[:, 5] = t**5\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(u, w, vt) = np.linalg.svd(A, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for example at the eigenvalues! They span almost 25 orders of magnitude! The condition number is the ratio of the highest to lowest eigenvalues, so it is of order $10^{25}$, which is \"not great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainv = vt.transpose().dot(np.diag(1. / w)).dot(u.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ainv.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients do not look correct at all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = A.dot(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed they do not predict the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, b, '.', label='data')\n",
    "plt.plot(t, bm, '.', label='model')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, b - bm, '.', label='data - model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\Delta$b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has happened? The design matrix $A$ has a large condition number, which is the ratio of the maximum to minimum eigenvalue. This is basically saying that the matrix is close to degenerate. When the condition number exceeds the dynamic range of your floating point numbers, there is no guarantee that the SVD solution will be stable. Basically, round-off error can then cause the procedure to fail.\n",
    "\n",
    "But we can avoid this situation and find the solution to our problem. We can rescale:\n",
    "\n",
    "$$t' = \\frac{t - {\\bar t}}{\\sigma_t}$$\n",
    "\n",
    "where in this case we have chosen to shift to a variable taken around the mean ${\\bar t}$ and to normalize by the standard deviation $\\sigma_t$. There is not anything particularly special about that exact choice. Anything that gets $t'$ to be order unity is fine.\n",
    "\n",
    "You can see that now when you expand this as a polynomial, all the powers of $t'$ are of the same order. This leads to a much better conditioned matrix. Then instead of solving for some polynomial $f(t)$ you solve for the polynomial solution $f'(t')$ in $t'$. This solution gives precisely the same values same function you would have gotten if you solved for $t$, in the sense that $f'(t') = f(t)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = (t - t.mean()) / t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((len(tp), 6))\n",
    "A[:, 0] = 1.\n",
    "A[:, 1] = tp \n",
    "A[:, 2] = tp**2\n",
    "A[:, 3] = tp**3\n",
    "A[:, 4] = tp**4\n",
    "A[:, 5] = tp**5\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(u, w, vt) = np.linalg.svd(A, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the eigenvalues. This is a much better conditioned matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainv = vt.transpose().dot(np.diag(1. / w)).dot(u.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ainv.dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are not the original coefficients. They are coefficients in the polynomial in $t'$. You can figure out the relationship between the two sets of coefficients, because they have a one-to-one relationship---they are both the same polynomial at the same order in $x$. But you rarely would really need to do that. If you need $f(t)$, you just determine $t'$ and plug it into $f'(t')$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = A.dot(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now they predict the data well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, b, '.', label='data')\n",
    "plt.plot(t, bm, '.', label='model')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('b')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y - ym, '.', label='data - model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\Delta$y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of the better-conditioned case is that the coefficients are less degenerate with each other.\n",
    "\n",
    "It is easy to see this for the case of fitting a line in $x$. If all the data is at very high $x$, then the slope and the offset are very degenerate. But once you rescale to $x'$, they are much more independently determined. \n",
    "\n",
    "This makes it clear that this effect can be important even if you are dealing with a non-linear model that cannot be addressed with SVD, or a linear model that for some reason you using a more generic minimizer on. In these cases, an inappropriate setup of the model can lead to unnecessarily degenerate parameters, which will cause any minimizer difficult. \n",
    "\n",
    "We can look at the covariance matrix of the parameters using the formula:\n",
    "\n",
    "$$C = (A^T\\cdot A)^{-1}$$\n",
    "\n",
    "to see this effect.\n",
    "\n",
    "For the demonstration, we won't look at a case that is literally degenerate, just a less-well-conditioned case. So we will fit a 3rd-order polynomial, and will use a dependent variable whose mean is not near zero for the data, but isn't so high that it causes numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(size=10000) * 5. + 10.\n",
    "y = 0.5 + 0.2 * x - 0.283 * x**2 + 0.0234 * x**3 + 1. * np.random.normal(size=len(x))\n",
    "plt.plot(x, y, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros((len(x), 4))\n",
    "A[:, 0] = 1.\n",
    "A[:, 1] = x \n",
    "A[:, 2] = x**2\n",
    "A[:, 3] = x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(u, w, vt) = np.linalg.svd(A, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainv = vt.transpose().dot(np.diag(1. / w)).dot(u.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ainv.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ym = A.dot(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit works just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, ym, '.', label='model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y - ym, '.', label='data - model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('$\\Delta$y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can look at the covariance matrix and see that it has large off-diagonal terms.\n",
    "\n",
    "The easiest way to see this is to actually look at the correlation matrix:\n",
    "\n",
    "$$\\frac{C_{ij}}{\\sqrt{C_{ii} C_{jj}}}$$\n",
    "\n",
    "which is limited to the range $-1$ to $1$, betweeen perfect anticorrelation and perfect correlation.\n",
    "\n",
    "The parameters of this fit are extremely highly correlated! Or anticorrelated!\n",
    "\n",
    "The SVD solution deals with this OK, but it is worth remembering that most general optimizers will have a much harder time with this situation. So when you are doing a general non-linear problem, it is even more important to avoid this type of situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.linalg.inv(A.T.dot(A))\n",
    "corr = np.zeros((4, 4))\n",
    "for i in np.arange(4):\n",
    "    for j in np.arange(4):\n",
    "        corr[i, j] = covar[i,j] / np.sqrt(covar[i, i] * covar[j, j])\n",
    "        \n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corr, vmin=-1, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue is resolved by transforming to $x'$, as I show below (though note that there are still significant covariances even in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ap = np.zeros((len(xp), 4))\n",
    "Ap[:, 0] = 1.\n",
    "Ap[:, 1] = xp \n",
    "Ap[:, 2] = xp**2\n",
    "Ap[:, 3] = xp**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covarp = np.linalg.inv(Ap.T.dot(Ap))\n",
    "corrp = np.zeros((4, 4))\n",
    "for i in np.arange(4):\n",
    "    for j in np.arange(4):\n",
    "        corrp[i, j] = covarp[i,j] / np.sqrt(covarp[i, i] * covarp[j, j])\n",
    "print(corrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corrp, vmin=-1, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
